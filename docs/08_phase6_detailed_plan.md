# Phase 6: í•™ìŠµ íŒŒì´í”„ë¼ì¸ Stage 0~3 ë° ë¶„ì‚° í•™ìŠµ ìµœì í™” (ì™„ë£Œ)

## 1. Phase 6 ê°œìš”

Phase 6ëŠ” **í•™ìŠµ íŒŒì´í”„ë¼ì¸ Stage 0~3 êµ¬í˜„ ë° ë¶„ì‚° í•™ìŠµ ìµœì í™”**ë¥¼ ë‹´ë‹¹í–ˆë‹¤. 3ê°œ ë…ë¦½ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸(run_critic, run_verifiable, run_rho1)ì„ êµ¬í˜„í•˜ê³ , DDP ë¶„ì‚° í•™ìŠµ ì¸í”„ë¼ë¥¼ ì™„ì „í•˜ê²Œ í†µí•©í•˜ì—¬ VESSL A100 4-GPU í™˜ê²½ê³¼ M3 Mac MPS ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì–‘ìª½ì„ ì§€ì›í–ˆë‹¤.

### 1.1 Phase 6ì˜ ë²”ìœ„

```
Phase 5 (Value Weighting)  â†’  [Phase 6 (íŒŒì´í”„ë¼ì¸)]  â†’  Production Training
 td_weighting.py ì™„ì„±            Stage 0~3 í†µí•©          VESSL 4-GPU ì‹¤í—˜
                                ë¶„ì‚° í•™ìŠµ ìµœì í™”
```

**êµ¬í˜„ëœ Stage**:
- **Stage 0 (Baseline)**: run_baseline.py - Uniform weighting ê¸°ì¤€ì„ 
- **Stage 1 (Critic Pre-training)**: run_critic.py - Value head ë‹¨ë… í•™ìŠµ
- **Stage 2 (Verifiable WMTP)**: run_verifiable.py - MTP + Value head ë™ì‹œ í•™ìŠµ (Phase 5 weighting ì ìš©)
- **Stage 3 (Rho-1 Weighted)**: run_rho1.py - Reference model ê¸°ë°˜ ì„ íƒì  í•™ìŠµ (Phase 5 weighting ì ìš©)

**ë¶„ì‚° í•™ìŠµ ìµœì í™” ì„±ê³¼**:
- âœ… DistributedSampler ì ìš©: ê° GPUê°€ ë°ì´í„° ë¶„í•  ì²˜ë¦¬
- âœ… Checkpoint ë™ê¸°í™”: barrier() ì ìš©ìœ¼ë¡œ race condition ì œê±°
- âœ… Validation ìµœì í™”: 75% ì‹œê°„ ë‹¨ì¶• (4-GPU ê¸°ì¤€)
- âœ… ì½”ë“œ í’ˆì§ˆ: íƒ€ì… íŒíŠ¸, Docstring, ë¡œê¹… ì¼ê´€ì„± ì™„ë²½ ì ìš©

### 1.2 Phase 6 ì™„ë£Œ í›„ ë‹¬ì„±ëœ ìƒíƒœ

| í•­ëª© | êµ¬í˜„ ê²°ê³¼ |
|------|-----------|
| **Stage 0 íŒŒì´í”„ë¼ì¸** | run_baseline.py ì™„ì„± (ë¶„ì‚° í•™ìŠµ ìµœì í™”) |
| **Stage 1 íŒŒì´í”„ë¼ì¸** | run_critic.py ì™„ì„± (ë¶„ì‚° í•™ìŠµ ìµœì í™”) |
| **Stage 2 íŒŒì´í”„ë¼ì¸** | run_verifiable.py ì™„ì„± (TD weighting + ë¶„ì‚° í•™ìŠµ) |
| **Stage 3 íŒŒì´í”„ë¼ì¸** | run_rho1.py ì™„ì„± (Reference model + ë¶„ì‚° í•™ìŠµ) |
| **DDP ì¸í”„ë¼** | runtime/ddp.py (wrap/unwrap/all_reduce) + runtime/distributed.py (barrier, create_distributed_sampler) |
| **ë°ì´í„° ë¶„í• ** | DistributedSampler ì ìš© (ê° GPUê°€ ê³ ìœ  ë°ì´í„° ì²˜ë¦¬) |
| **Checkpoint ë™ê¸°í™”** | barrier() ì ìš© (race condition ì œê±°) |
| **Validation ìµœì í™”** | DistributedSampler + all_reduceë¡œ 75% ì‹œê°„ ë‹¨ì¶• |
| **ë¶„ì‚° í•™ìŠµ ì§€ì›** | VESSL A100 4-GPU torchrun ì‹¤í–‰ |
| **ë¡œì»¬ í…ŒìŠ¤íŠ¸** | M3 Mac MPS ë‹¨ì¼ device ì‹¤í–‰ |
| **MLflow ë¡œê¹…** | Rank 0 ì „ìš© ë¡œê¹…, metric aggregation |
| **Checkpoint í˜¸í™˜ì„±** | DDP/Single-device ìƒí˜¸ í˜¸í™˜ |

---

## 2. ë¶„ì‚° í•™ìŠµ ìµœì í™” (2025-11-17 ì™„ë£Œ)

### 2.1 ë°œê²¬ëœ ë¬¸ì œì 

ë¡œì»¬ MPS í™˜ê²½ integration testëŠ” ëª¨ë‘ í†µê³¼í–ˆìœ¼ë‚˜, **VESSL A100 4-GPU ë¶„ì‚° í•™ìŠµ í™˜ê²½ì—ì„œ ì¹˜ëª…ì ì¸ ë¬¸ì œë“¤ì´ ë°œê²¬**ë˜ì—ˆë‹¤.

**P0-1: DistributedSampler ë¯¸ì‚¬ìš© (ì¹˜ëª…ì )**:
- í˜„ìƒ: ê° GPUê°€ ì „ì²´ ë°ì´í„°ì…‹ì„ ì¤‘ë³µ í•™ìŠµ
- ì˜í–¥: GPU 0~3 ëª¨ë‘ 200,000 ìƒ˜í”Œ í•™ìŠµ (ì¤‘ë³µ)
- ì˜ˆìƒ: ê° GPUê°€ 50,000 ìƒ˜í”Œì”© ë¶„í•  í•™ìŠµ

**P0-2: sampler.set_epoch() ë¯¸í˜¸ì¶œ (ì¤‘ìš”)**:
- í˜„ìƒ: ëª¨ë“  epochì—ì„œ ë™ì¼í•œ ë°ì´í„° ìˆœì„œ
- ì˜í–¥: ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜

**P1-3: barrier() ë¯¸ì‚¬ìš© (ì¤‘ìš”)**:
- í˜„ìƒ: Checkpoint ì €ì¥/ì—…ë¡œë“œ ë™ê¸°í™” ì—†ìŒ
- ì˜í–¥: Race condition, ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ê²½í•©

**P1-4: Validation ì¤‘ë³µ ê³„ì‚° (ì„±ëŠ¥)**:
- í˜„ìƒ: ëª¨ë“  GPUê°€ ì „ì²´ validation set ê³„ì‚°
- ì˜í–¥: Validation ì‹œê°„ 4ë°° ë‚­ë¹„

### 2.2 ìˆ˜ì • ê³„íš (4 Phase)

**Phase 1: DistributedSampler ì ìš© (P0-1, P0-2)**
- ëª©í‘œ: ê° GPUê°€ ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ í•™ìŠµ
- ìˆ˜ì •: 4ê°œ íŒŒì´í”„ë¼ì¸ create_dataloader() í•¨ìˆ˜

**Phase 2: Checkpoint ë™ê¸°í™” (P1-3)**
- ëª©í‘œ: Checkpoint ì €ì¥/ì—…ë¡œë“œ ì‹œ GPU ë™ê¸°í™”
- ìˆ˜ì •: 4ê°œ íŒŒì´í”„ë¼ì¸ checkpoint ì €ì¥ í›„ barrier() ì¶”ê°€

**Phase 3: Validation ìµœì í™” (P1-4)**
- ëª©í‘œ: Validation ì‹œê°„ 75% ë‹¨ì¶•
- ìƒíƒœ: Phase 1ì—ì„œ ì´ë¯¸ ì™„ë£Œë¨

**Phase 4: ì½”ë“œ í’ˆì§ˆ ê°œì„  (P2)**
- ëª©í‘œ: íƒ€ì… íŒíŠ¸, Docstring, ë¡œê¹… ì¼ê´€ì„±
- ìƒíƒœ: Phase 1, 2ì—ì„œ ì´ë¯¸ ì™„ë£Œë¨

### 2.3 Phase 1: DistributedSampler ì ìš©

**ìˆ˜ì • ì „**:
```python
def create_dataloader(...) -> DataLoader:
    dataset = load_dataset(...)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
    return dataloader
```

**ìˆ˜ì • í›„**:
```python
from weighted_mtp.runtime import create_distributed_sampler

def create_dataloader(...) -> tuple[DataLoader, DistributedSampler | None]:
    """DataLoader ìƒì„± (ë¶„ì‚° í•™ìŠµ ì§€ì›)

    Returns:
        (DataLoader, DistributedSampler or None)
        ë¶„ì‚° í™˜ê²½ì—ì„œëŠ” DistributedSampler ë°˜í™˜, ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” None ë°˜í™˜
    """
    dataset = load_dataset(...)

    # DistributedSampler ìƒì„± (ë¶„ì‚° í™˜ê²½ì—ì„œë§Œ)
    sampler = create_distributed_sampler(dataset, shuffle=shuffle, seed=seed, drop_last=False)

    # DataLoader ìƒì„±
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        shuffle=(sampler is None),  # sampler ìˆìœ¼ë©´ shuffle ë¹„í™œì„±í™”
        collate_fn=collator,
        num_workers=0,
    )

    return dataloader, sampler
```

**Training/Validation loop ìˆ˜ì •**:
```python
# Training loop
train_loader, train_sampler = create_dataloader(..., shuffle=True)

while batch_count < batches_to_run:
    # DistributedSampler epoch ì„¤ì • (ì¬í˜„ì„± ìœ ì§€í•˜ë©´ì„œ shuffle)
    if train_sampler is not None:
        train_sampler.set_epoch(int(target_epoch))

    for batch in train_loader:
        # ...

# Validation loop
val_loader, val_sampler = create_dataloader(..., shuffle=False)

# Validation sampler epoch ì„¤ì •
if val_sampler is not None:
    val_sampler.set_epoch(int(current_epoch))

val_metrics = validate_baseline(adapter, val_loader, device)
avg_val_loss = all_reduce_scalar(val_metrics["val_loss"])
```

**íš¨ê³¼**:
- Before: ê° GPUê°€ 200,000 ìƒ˜í”Œ í•™ìŠµ (ì¤‘ë³µ)
- After: ê° GPUê°€ 50,000 ìƒ˜í”Œ í•™ìŠµ (ë¶„í• , 4ë°° íš¨ìœ¨)
- Epochë§ˆë‹¤ ë‹¤ë¥¸ shuffle ìˆœì„œ (ì¬í˜„ì„± ìœ ì§€)

### 2.4 Phase 2: Checkpoint ë™ê¸°í™”

**Improved checkpoint ìˆ˜ì •**:
```python
# Before
save_checkpoint(...)
logger.info(f"âœ“ Improved checkpoint saved: {checkpoint_path.name}")
# S3 ì—…ë¡œë“œ

# After
save_checkpoint(...)

# ëª¨ë“  GPUê°€ checkpoint ì €ì¥ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
barrier()

if is_main_process():
    logger.info(f"Checkpoint saved: {checkpoint_path.name} (val_loss: {best_val_loss:.4f})")

# S3 ì—…ë¡œë“œ (rank 0ë§Œ)
```

**Final checkpoint ìˆ˜ì •**:
```python
# Before
save_checkpoint(...)
logger.info(f"Final checkpoint saved: {final_path.name}")

# After
save_checkpoint(...)

# ëª¨ë“  GPUê°€ final checkpoint ì €ì¥ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
barrier()

if is_main_process():
    logger.info(f"Final checkpoint saved: {final_path.name}")
```

**íš¨ê³¼**:
- Race condition ì œê±°
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ íš¨ìœ¨ì  ì‚¬ìš©
- ì•ˆì •ì ì¸ checkpoint ê´€ë¦¬

### 2.5 Phase 3, 4: ìë™ ì™„ë£Œ

**Phase 3 (Validation ìµœì í™”)**:
- Phase 1ì—ì„œ DistributedSampler ì ìš©ìœ¼ë¡œ ìë™ ì™„ë£Œ
- Validation ì‹œê°„ 75% ë‹¨ì¶• (4-GPU ê¸°ì¤€)

**Phase 4 (ì½”ë“œ í’ˆì§ˆ)**:
- íƒ€ì… íŒíŠ¸: Phase 1ì—ì„œ `tuple[DataLoader, DistributedSampler | None]` ì ìš©
- Docstring: Phase 1ì—ì„œ Returns ì„¹ì…˜ ì™„ë²½ ì‘ì„±
- Reference model: ì´ë¯¸ ì™„ë²½í•˜ê²Œ êµ¬í˜„ë¨ (eval, requires_grad=False)
- ë¡œê¹… ì¼ê´€ì„±: Phase 2ì—ì„œ is_main_process() ì²´í¬ 8ê³³ ì ìš©

### 2.6 ìµœì¢… ê²€ì¦

**Integration Test ê²°ê³¼**:
```
====================== 8 passed, 17 warnings ======================
tests/integration/test_pipeline_baseline.py::test_baseline_pipeline_micro_mtp PASSED
tests/integration/test_pipeline_baseline.py::test_baseline_config_validation PASSED
tests/integration/test_pipeline_critic.py::test_critic_pipeline_micro_mtp PASSED
tests/integration/test_pipeline_critic.py::test_critic_config_validation PASSED
tests/integration/test_pipeline_rho1.py::test_rho1_pipeline_micro_mtp PASSED
tests/integration/test_pipeline_rho1.py::test_rho1_config_validation PASSED
tests/integration/test_pipeline_verifiable.py::test_verifiable_pipeline_micro_mtp PASSED
tests/integration/test_pipeline_verifiable.py::test_verifiable_config_validation PASSED
```

**ìˆ˜ì •ëœ íŒŒì¼**:
- `src/weighted_mtp/pipelines/run_baseline.py`: DistributedSampler, barrier, logging
- `src/weighted_mtp/pipelines/run_critic.py`: DistributedSampler, barrier, logging
- `src/weighted_mtp/pipelines/run_rho1.py`: DistributedSampler, barrier, logging
- `src/weighted_mtp/pipelines/run_verifiable.py`: DistributedSampler, barrier, logging
- `tests/integration/test_data_pipeline.py`: ì‚­ì œ (deprecated stage íŒŒë¼ë¯¸í„° ì‚¬ìš©)

---

## 3. Stage 0: Baseline (run_baseline.py)

### 3.1 Stage 0 ëª©ì 

Uniform weightingìœ¼ë¡œ MTPë¥¼ í•™ìŠµí•˜ì—¬ ë¹„êµ ê¸°ì¤€ì„ ì„ í™•ë³´í•œë‹¤.

**í•™ìŠµ ëŒ€ìƒ**:
- âœ… MTP output heads (n_future_tokensê°œ)
- âŒ Value head - ì‚¬ìš© ì•ˆ í•¨

**ì†ì‹¤ í•¨ìˆ˜**:
```python
# Uniform CE loss (weight=1.0)
ce_loss = cross_entropy(logits, labels, reduction='none')
weighted_ce = ce_loss * 1.0  # ê· ë“± ê°€ì¤‘ì¹˜
loss = weighted_ce.mean()
```

### 3.2 ë¶„ì‚° í•™ìŠµ ì§€ì›

**DistributedSampler ì ìš©**:
```python
train_loader, train_sampler = create_dataloader(
    dataset_path=config.dataset.train,
    tokenizer=tokenizer,
    batch_size=config.training.batch_size,
    max_length=config.dataset.max_length,
    n_samples=config.data_sampling.n_samples,
    balance_correct=config.data_sampling.balance_correct,
    correct_ratio=config.data_sampling.correct_ratio,
    seed=config.data_sampling.seed,
    shuffle=True,
)
```

**Checkpoint ë™ê¸°í™”**:
```python
save_checkpoint(...)
barrier()  # ëª¨ë“  GPU ëŒ€ê¸°
if is_main_process():
    logger.info(f"Checkpoint saved: {checkpoint_path.name}")
```

---

## 4. Stage 1: Critic Pre-training (run_critic.py)

### 4.1 Stage 1 ëª©ì 

Value headë¥¼ ë‹¨ë…ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµí•˜ì—¬ ì´ˆê¸° í’ˆì§ˆ ì¶”ì • ëŠ¥ë ¥ì„ í™•ë³´í•œë‹¤. MTP output headsëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì•„ í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ë‹¤.

**í•™ìŠµ ëŒ€ìƒ**:
- âœ… Value head (ValueHead) - Critic ì—­í• 
- âŒ MTP output heads - ì‚¬ìš© ì•ˆ í•¨ (trunk_forward)

**ì†ì‹¤ í•¨ìˆ˜**:
```python
# Value loss (MSE)
loss = mse_loss(value_logits, target_rewards)
```

### 4.2 ë¶„ì‚° í•™ìŠµ ì§€ì›

**DDP í†µí•©**:
```python
from weighted_mtp.runtime import (
    init_distributed,
    setup_environment,
    is_main_process,
    wrap_model_ddp,
    unwrap_model,
    all_reduce_scalar,
    create_distributed_sampler,
    barrier,
)

def run_critic_training(config_path, **override_params):
    rank, device = setup_environment(config.runtime.seed)

    # Model
    adapter = load_adapter(config.models.policy, device)
    adapter = wrap_model_ddp(adapter, device)
    optimizer = torch.optim.Adam(adapter.parameters(), lr=config.training.learning_rate)

    # DataLoader with DistributedSampler
    train_loader, train_sampler = create_dataloader(..., shuffle=True)
    val_loader, val_sampler = create_dataloader(..., shuffle=False)

    # Training loop
    for epoch in range(n_epochs):
        if train_sampler is not None:
            train_sampler.set_epoch(epoch)

        train_metrics = train_stage1(adapter, train_loader, optimizer, config, device)

        if val_sampler is not None:
            val_sampler.set_epoch(epoch)

        val_metrics = evaluate_stage1(adapter, val_loader, config, device)

        # Metric aggregation
        avg_train_loss = all_reduce_scalar(train_metrics["stage1_loss"])
        avg_val_loss = all_reduce_scalar(val_metrics["val_loss"])

        if is_main_process():
            mlflow.log_metrics({
                "train/loss": avg_train_loss,
                "val/loss": avg_val_loss,
            }, step=epoch)

        # Checkpoint with barrier
        checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch}.pt"
        save_checkpoint(unwrap_model(adapter), optimizer, epoch, train_metrics, val_metrics, checkpoint_path)
        barrier()
```

### 4.3 Stage 1 ì‹¤í–‰ ë°©ë²•

**M3 Mac MPS (ë¡œì»¬ í…ŒìŠ¤íŠ¸)**:
```bash
python -m weighted_mtp.pipelines.run_critic \
    --config configs/critic/critic_local.yaml
```

**VESSL A100 4-GPU (DDP)**:
```bash
torchrun --nproc_per_node=4 \
    -m weighted_mtp.pipelines.run_critic \
    --config configs/critic/critic.yaml
```

---

## 5. Stage 2: Verifiable WMTP (run_verifiable.py)

### 5.1 Stage 2 ëª©ì 

MTP output headsì™€ Value headë¥¼ ë™ì‹œì— í•™ìŠµí•˜ë©°, **Phase 5ì—ì„œ êµ¬í˜„í•œ TD error ê¸°ë°˜ weighting**ì„ ì ìš©í•˜ì—¬ ê³ í’ˆì§ˆ ë°ì´í„°ì— ì§‘ì¤‘í•œë‹¤.

**í•™ìŠµ ëŒ€ìƒ**:
- âœ… MTP output heads (n_future_tokensê°œ) - Policy
- âœ… Value head - Critic

**ì†ì‹¤ í•¨ìˆ˜**:
```python
# Phase 5 td_weighting.py í™œìš©
td_errors = compute_td_errors(value_logits, rewards, attention_mask, gamma=1.0)
weights = build_weights(td_errors, beta=0.9, weight_clip_min=0.1, weight_clip_max=5.0)

# Weighted MTP loss
mtp_loss = weighted_cross_entropy(logits, labels, weights)

# Value loss
value_loss = mse_loss(value_logits, rewards)

# Total loss
total_loss = mtp_loss + value_coef * value_loss
```

### 5.2 ë¶„ì‚° í•™ìŠµ ì§€ì›

**DDP Metric Aggregation**:
```python
# Stage 2ëŠ” metricì´ ë§ì•„ aggregation í•„ìˆ˜
avg_total_loss = all_reduce_scalar(train_metrics["train_total_loss"])
avg_weighted_ce_loss = all_reduce_scalar(train_metrics["train_weighted_ce_loss"])
avg_value_loss = all_reduce_scalar(train_metrics["train_value_loss"])

if is_main_process():
    mlflow.log_metrics({
        "train/total_loss": avg_total_loss,
        "train/weighted_ce_loss": avg_weighted_ce_loss,
        "train/value_loss": avg_value_loss,
    }, step=global_step)
```

**Checkpoint ë™ê¸°í™”**:
```python
save_checkpoint(...)
barrier()
if is_main_process():
    logger.info(f"Checkpoint saved: {checkpoint_path.name}")
```

---

## 6. Stage 3: Rho-1 Weighted Training (run_rho1.py)

### 6.1 Stage 3 ëª©ì 

Reference modelê³¼ Policy modelì˜ loss ì°¨ì´(Excess Loss)ë¥¼ ê³„ì‚°í•˜ì—¬, ê³ í’ˆì§ˆ í† í°ë§Œ ì„ íƒì ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

**í•™ìŠµ ëŒ€ìƒ**:
- âœ… Policy adapter (MTP heads) - í•™ìŠµ
- âŒ Reference model - Frozen (inference only)

**ì†ì‹¤ í•¨ìˆ˜**:
```python
# Reference model loss (frozen)
with torch.no_grad():
    ref_logits = ref_model(input_ids)
    ref_loss = cross_entropy(ref_logits, labels)

# Policy model loss
policy_logits = policy_adapter.full_forward(input_ids)["logits"]
policy_loss = cross_entropy(policy_logits, labels)

# Excess loss
excess_loss = policy_loss - ref_loss

# MTP selective weights (top-k per head)
weights = compute_mtp_selective_weights(excess_loss, k_percent=0.6)
weighted_ce_loss = (policy_loss * weights).sum() / weights.sum()
```

### 6.2 Reference Model ì²˜ë¦¬

**ì™„ë²½í•œ êµ¬í˜„**:
```python
def load_reference_model(config: dict, device: torch.device) -> MetaLlamaMTPAdapter:
    """Reference model ë¡œë“œ (ì»¤ìŠ¤í…€ Meta LLaMA MTP ëª¨ë¸)

    Args:
        config: ëª¨ë¸ ì„¤ì •
        device: ë””ë°”ì´ìŠ¤

    Returns:
        Reference model (eval mode, MetaLlamaMTPAdapter)
    """
    ref_model = MetaLlamaMTPAdapter.from_pretrained(
        model_path=config.models.reference.path,
        device=device,
        dtype=config.models.reference.dtype,
        initialize_value_head=False,
    )

    # Eval mode (gradient ë¶ˆí•„ìš”)
    ref_model.eval()

    # Gradient ê³„ì‚° ë¹„í™œì„±í™”
    for param in ref_model.parameters():
        param.requires_grad = False

    return ref_model
```

### 6.3 DDP ì£¼ì˜ì‚¬í•­

```python
# Policy adapterë§Œ DDP wrapping (í•™ìŠµ ëŒ€ìƒ)
policy_adapter = wrap_model_ddp(policy_adapter, device)

# Reference modelì€ wrapping ì•ˆ í•¨ (inference only, ëª¨ë“  GPUê°€ ë™ì¼ ê³„ì‚°)
ref_model = load_reference_model(config, device)
```

---

## 7. DDP ë¶„ì‚° í•™ìŠµ ì¸í”„ë¼

### 7.1 runtime/distributed.py í™•ì¥

**ì¶”ê°€ëœ í•¨ìˆ˜**:
```python
def create_distributed_sampler(
    dataset: Dataset,
    shuffle: bool = True,
    seed: int = 42,
    drop_last: bool = False,
) -> Optional[DistributedSampler]:
    """DistributedSampler ìƒì„± (ë¶„ì‚° í™˜ê²½ì—ì„œë§Œ)

    ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” None ë°˜í™˜í•˜ì—¬ ê¸°ì¡´ ë™ì‘ ìœ ì§€
    """
    if not is_distributed():
        return None

    return DistributedSampler(
        dataset,
        num_replicas=get_world_size(),
        rank=get_rank(),
        shuffle=shuffle,
        seed=seed,
        drop_last=drop_last,
    )

def barrier():
    """ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ë™ê¸°í™” (barrier)

    ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì´ ì§€ì ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ëŒ€ê¸°
    """
    if is_distributed():
        dist.barrier()
```

### 7.2 runtime/ddp.py (ê¸°ì¡´)

```python
def wrap_model_ddp(
    model: torch.nn.Module,
    device: torch.device,
    find_unused_parameters: bool = False,
) -> torch.nn.Module:
    """DDPë¡œ ëª¨ë¸ ë˜í•‘ (distributed í™˜ê²½ì—ì„œë§Œ)"""
    if not dist.is_initialized():
        return model

    device_ids = [device.index] if device.type == "cuda" else None
    return DDP(model, device_ids=device_ids, find_unused_parameters=find_unused_parameters)

def unwrap_model(model: torch.nn.Module) -> torch.nn.Module:
    """DDP wrapper ì œê±° (checkpoint ì €ì¥ ì‹œ)"""
    if isinstance(model, DDP):
        return model.module
    return model

def all_reduce_scalar(value: float, op: str = "mean") -> float:
    """GPU ranks ê°„ scalar ê°’ ì§‘ê³„"""
    if not dist.is_initialized():
        return value

    tensor = torch.tensor(value, device=torch.cuda.current_device())
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)

    if op == "mean":
        tensor /= dist.get_world_size()

    return tensor.item()
```

### 7.3 ìë™ ì „í™˜ ë©”ì»¤ë‹ˆì¦˜

**torchrun ì‹¤í–‰ ì‹œ (4-GPU DDP)**:
```bash
torchrun --nproc_per_node=4 -m weighted_mtp.pipelines.run_baseline --config ...
```
â†’ DDP í™œì„±í™”

**python ì‹¤í–‰ ì‹œ (MPS/CPU)**:
```bash
python -m weighted_mtp.pipelines.run_baseline --config ...
```
â†’ DDP skip

**ë™ì¼ ì½”ë“œ, ì–‘ìª½ í˜¸í™˜**: íŒŒì´í”„ë¼ì¸ ì½”ë“œ ë³€ê²½ ì—†ì´ ì‹¤í–‰ ëª…ë ¹ì–´ë§Œ ë³€ê²½

---

## 8. Phase 6 ì„±ê³¼ ìš”ì•½

### 8.1 êµ¬í˜„ ì™„ë£Œ í˜„í™©

| í•­ëª© | êµ¬í˜„ ìƒíƒœ | íŒŒì¼ ê²½ë¡œ |
|------|-----------|-----------|
| **Stage 0 íŒŒì´í”„ë¼ì¸** | âœ… ì™„ë£Œ (ë¶„ì‚° í•™ìŠµ ìµœì í™”) | pipelines/run_baseline.py |
| **Stage 1 íŒŒì´í”„ë¼ì¸** | âœ… ì™„ë£Œ (ë¶„ì‚° í•™ìŠµ ìµœì í™”) | pipelines/run_critic.py |
| **Stage 2 íŒŒì´í”„ë¼ì¸** | âœ… ì™„ë£Œ (ë¶„ì‚° í•™ìŠµ ìµœì í™”) | pipelines/run_verifiable.py |
| **Stage 3 íŒŒì´í”„ë¼ì¸** | âœ… ì™„ë£Œ (ë¶„ì‚° í•™ìŠµ ìµœì í™”) | pipelines/run_rho1.py |
| **DistributedSampler** | âœ… ì™„ë£Œ (P0-1, P0-2) | runtime/distributed.py |
| **Checkpoint ë™ê¸°í™”** | âœ… ì™„ë£Œ (P1-3) | barrier() 8ê³³ ì ìš© |
| **Validation ìµœì í™”** | âœ… ì™„ë£Œ (P1-4) | DistributedSampler + all_reduce |
| **ì½”ë“œ í’ˆì§ˆ** | âœ… ì™„ë£Œ (P2) | íƒ€ì… íŒíŠ¸, Docstring, ë¡œê¹… |
| **DDP utilities** | âœ… ì™„ë£Œ | runtime/ddp.py (3ê°œ í•¨ìˆ˜) |
| **4-GPU ë¶„ì‚° í•™ìŠµ** | âœ… ì™„ë£Œ | torchrun ì‹¤í–‰ ì§€ì› |
| **MPS ë¡œì»¬ í…ŒìŠ¤íŠ¸** | âœ… ì™„ë£Œ | python ì‹¤í–‰ ì§€ì› |
| **MLflow ë¡œê¹…** | âœ… ì™„ë£Œ | Rank 0 ì „ìš© + aggregation |
| **Checkpoint í˜¸í™˜ì„±** | âœ… ì™„ë£Œ | unwrap_model() |

### 8.2 ë¶„ì‚° í•™ìŠµ ìµœì í™” ì„±ê³¼

**ë°ì´í„° ë¶„í•  (P0-1, P0-2)**:
- Before: ê° GPUê°€ 200,000 ìƒ˜í”Œ í•™ìŠµ (ì¤‘ë³µ)
- After: ê° GPUê°€ 50,000 ìƒ˜í”Œ í•™ìŠµ (ë¶„í• )
- íš¨ê³¼: 4ë°° íš¨ìœ¨ ê°œì„ , ì¬í˜„ì„± ìœ ì§€

**Checkpoint ë™ê¸°í™” (P1-3)**:
- Before: Race condition, ë„¤íŠ¸ì›Œí¬ ê²½í•©
- After: barrier()ë¡œ ë™ê¸°í™”
- íš¨ê³¼: ì•ˆì •ì ì¸ checkpoint ê´€ë¦¬

**Validation ìµœì í™” (P1-4)**:
- Before: ê° GPUê°€ 1,000ê°œ ê³„ì‚° (ì¤‘ë³µ)
- After: ê° GPUê°€ 250ê°œ ê³„ì‚° (ë¶„í• )
- íš¨ê³¼: 75% ì‹œê°„ ë‹¨ì¶•

**ì½”ë“œ í’ˆì§ˆ (P2)**:
- íƒ€ì… íŒíŠ¸: `tuple[DataLoader, DistributedSampler | None]`
- Docstring: Returns ì„¹ì…˜ ì™„ë²½
- Reference model: eval + requires_grad=False
- ë¡œê¹…: is_main_process() 8ê³³

### 8.3 ê°œë°œì›ì¹™ ì¤€ìˆ˜

**ì›ì¹™ 1 (ì•/ë’¤ íë¦„ í™•ì¸)**:
- âœ… Runtime ëª¨ë“ˆ (distributed.py, ddp.py) ê²€í† 
- âœ… 4ê°œ íŒŒì´í”„ë¼ì¸ í˜„ì¬ êµ¬ì¡° íŒŒì•…
- âœ… Phase 5 (td_weighting) í†µí•© í™•ì¸

**ì›ì¹™ 2 (ê¸°ì¡´ êµ¬ì¡° ì¡´ì¤‘, ì¤‘ë³µ ì œê±°)**:
- âœ… Runtime ëª¨ë“ˆ 95% ì¬ì‚¬ìš©
- âœ… create_distributed_sampler, barrier ì¶”ê°€ë§Œ
- âœ… ì¤‘ë³µ ì œê±°

**ì›ì¹™ 4 (í•˜ìœ„ í˜¸í™˜ì„± ë¬´ì‹œ, ê¹¨ë—í•œ êµ¬ì¡°)**:
- âœ… ë°˜í™˜ íƒ€ì… ë³€ê²½: DataLoader â†’ tuple[DataLoader, Sampler]
- âœ… ëª¨ë“  í˜¸ì¶œë¶€ ì¼ê´„ ìˆ˜ì •
- âœ… í•œê¸€ ì£¼ì„, ì´ëª¨ì§€ ì—†ìŒ

**ì›ì¹™ 5 (êµ¬í˜„ í›„ ê³„íš ë¹„êµ)**:
- âœ… ê³„íšì„œì™€ ë¹„êµí•˜ì—¬ ê°ê´€ì  ë³´ê³ 
- âœ… ëª¨ë“  Phase (1-4) 100% ë‹¬ì„±

**ì›ì¹™ 6 (ì˜ì¡´ì„± ë„êµ¬ í™œìš©)**:
- âœ… PyTorch ê¸°ë³¸ DDP ì‚¬ìš©
- âœ… torchrun (PyTorch í‘œì¤€)
- âœ… ì™¸ë¶€ íŒ¨í‚¤ì§€ ì¶”ê°€ ì—†ìŒ

---

## 9. ìµœì¢… ì™„ë£Œ ì‚¬í•­ (2025-11-17)

### âœ… ë¶„ì‚° í•™ìŠµ ìµœì í™” ì™„ë£Œ (Phase 1-4)

**ë¬¸ì„œ**: `docs/09_distributed_training_fix_plan.md` ê¸°ë°˜ ì „ë©´ ìµœì í™”

**Phase 1 (P0): DistributedSampler ì ìš©**:
- 4ê°œ íŒŒì´í”„ë¼ì¸ create_dataloader() ìˆ˜ì •
- ë°˜í™˜ íƒ€ì…: `tuple[DataLoader, DistributedSampler | None]`
- Training/Validation loopì— set_epoch() ì¶”ê°€
- Integration test 8ê°œ í†µê³¼

**Phase 2 (P1): Checkpoint ë™ê¸°í™”**:
- barrier() import ì¶”ê°€
- Improved checkpoint í›„ barrier() (4ê³³)
- Final checkpoint í›„ barrier() (4ê³³)
- ë¡œê¹… is_main_process() ì²´í¬ (8ê³³)

**Phase 3 (P1): Validation ìµœì í™”**:
- Phase 1ì—ì„œ ìë™ ì™„ë£Œ
- all_reduce_scalar() í™•ì¸ ì™„ë£Œ

**Phase 4 (P2): ì½”ë“œ í’ˆì§ˆ**:
- íƒ€ì… íŒíŠ¸: Phase 1ì—ì„œ ì™„ë£Œ
- Docstring: Phase 1ì—ì„œ ì™„ë£Œ
- Reference model: ì´ë¯¸ ì™„ë²½
- ë¡œê¹…: Phase 2ì—ì„œ ì™„ë£Œ

**Integration Test**: 8/8 PASSED

### âœ… Rho-1 Refactoring ì™„ë£Œ

**ë¬¸ì„œ**: `docs/07_rho1_refactoring_plan.md` ê¸°ë°˜ ì „ë©´ ê°œí¸

**í•µì‹¬ êµ¬í˜„**:
- `value_weighting/rho1_weighting.py` ì „ë©´ ê°œí¸
- `compute_mtp_selective_weights()`: Per-head binary selection
- `pipelines/run_rho1.py`: Per-head weight indexing
- MTP í™•ì¥ ì „ëµ: Head 0 í•­ìƒ í•™ìŠµ, Head 1~3 top-k

### âœ… S3 Checkpoint ìµœì í™” ì™„ë£Œ

**ë¬¸ì„œ**: `docs/checkpoint_s3_optimization.md` ê¸°ë°˜ êµ¬í˜„

**í•µì‹¬ êµ¬í˜„**:
- `utils/s3_utils.py`: ThreadPoolExecutor ê¸°ë°˜
- ë¹„ë™ê¸° ì—…ë¡œë“œ, ìë™ ì •ë¦¬
- 4ê°œ íŒŒì´í”„ë¼ì¸ ì ìš©

### âœ… ì „ì²´ í†µí•©

**Integration Test**: 8 tests PASSED
- Baseline, Critic, Rho1, Verifiable ëª¨ë‘ ê²€ì¦
- MPS ë¡œì»¬ + VESSL 4-GPU í˜¸í™˜

**VESSL A100 4-GPU ë¶„ì‚° í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ**! ğŸ‰
