# Rho-1 WMTP (Reference-based Weighting)
# Excess loss weighting with Reference model
# 프로덕션 설정 (VESSL H200 4-GPU)

# 프로젝트 정보
project:
  name: weighted-mtp
  version: "2.0.0"

# 실험 메타정보
experiment:
  name: last-fianl-rho1
  description: "Rho-1 Weighted MTP with Reference Model"
  stage: rho1
  tags:
    - rho1
    - wmtp
    - reference-based
    - production

# 모델 설정
models:
  policy:
    name: meta-llama-mtp
    path: storage/models/meta-llama-mtp
    tokenizer_path: storage/models/meta-llama-mtp/tokenizer
    params:
      dim: 4096
      n_layers: 32
      n_heads: 32
      n_future_tokens: 4
      intermediate_size: 11008
      rope_theta: 10000.0
      vocab_size: 32000
    dtype: bfloat16

  # Rho-1 Reference model (LoRA checkpoint 또는 HuggingFace 디렉토리)
  # checkpoint_path:
  #   - .pt 파일: LoRA checkpoint (base_model_path + LoRA merge)
  #   - 디렉토리: HuggingFace 모델 (그대로 로드)
  reference:
    name: ref-sheared-llama-2.7b
    checkpoint_path: storage/checkpoints/ref-tuning/lora-ref-tuning/checkpoint_final.pt
    base_model_path: storage/models/ref-sheared-llama-2.7b/raw
    dtype: bfloat16

# 데이터셋 설정
dataset:
  name: codecontests
  train: storage/datasets/codecontests/processed/train.jsonl
  validation: storage/datasets/codecontests/processed/valid.jsonl
  max_length: 2048

# 데이터 샘플링
data_sampling:
  seed: 42
  val_n_samples: 500
  use_pairwise: false

  n_samples: 200000
  max_pairs_per_problem: 60  # problem당 최대 쌍 수 (다양성 확보, 과적합 방지)

  difficulty_bins:
    all: [1, 25]  # difficulty 0 제외
  difficulty_weights:
    all: 1.0

# 학습 설정 (H200 3-GPU 최적화)
training:
  n_epochs: 1.0
  batch_size: 32  # Per GPU (Reference model 메모리 고려)
  gradient_accumulation_steps: 1  # Effective batch size = 18 × 2 × 3 = 108
  learning_rate: 1.0e-4  # LoRA 사용 시 Full FT 대비 LR 높여야 함
  max_grad_norm: 1.0

  # Rho-1 weighting
  rho1_mode: "signed"  # "signed" (Policy > Ref만) / "absolute" (차이 큰 모든 토큰) / "difficult" (Ref가 어려워하는 토큰)
  temperature: 1.0  # Rho-1 temperature
  k_percent: 0.7  # Top-k threshold

  log_interval: 5

  # LoRA 설정 (All-Linear LoRA for CodeContests)
  use_lora: true
  lora:
    rank: 64           # 코드/논리 태스크는 64 권장
    alpha: 128.0       # rank의 2배
    dropout: 0.05      # 과적합 방지
    target_modules:    # 모든 Linear Layer에 적용
      - wq
      - wk
      - wv
      - wo
      - w1
      - w2
      - w3

  # Learning rate scheduler
  lr_scheduler:
    type: cosine
    warmup_ratio: 0.03
    min_lr_ratio: 0.005

# 체크포인트
checkpoint:
  save_dir: storage/checkpoints/rho1/${experiment.name}
  save_checkpoint_every: 0.2
  save_best: true
  save_final: true
  save_total_limit: 3
  s3_upload: false
  save_lora_only: true

# 런타임 설정
runtime:
  device: cuda
  seed: 42
  mixed_precision: true

# 분산학습 설정
distributed:
  fsdp:
    sharding_strategy: FULL_SHARD
    mixed_precision: true
    cpu_offload: false
    activation_checkpointing: true

# 스토리지
storage:
  root: storage
  models_dir: storage/models
  datasets_dir: storage/datasets
  checkpoints_dir: storage/checkpoints

# MLflow 추적 (로컬 파일 기반)
mlflow:
  tracking_uri: ""  # 빈 문자열 = ./mlruns 디렉터리 사용
  experiment: "weighted-mtp/production"

# 로깅
logging:
  level: INFO
  format: "%(asctime)s [%(levelname)s] [%(name)s] %(message)s"
