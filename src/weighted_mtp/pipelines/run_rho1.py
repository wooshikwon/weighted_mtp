"""Rho-1 WMTP Runner

ë…ë¦½ ì‹¤í–‰:
    python -m weighted_mtp.pipelines.run_rho1 --config configs/rho1/rho1.yaml
"""

import argparse
import logging
import os
from pathlib import Path
from typing import Any

import mlflow
import torch
import torch.nn.functional as F
from omegaconf import OmegaConf
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

from weighted_mtp.core.env import ensure_env_loaded
from weighted_mtp.core.logging import setup_logging
from weighted_mtp.data import AlpacaDataCollator, load_dataset
from weighted_mtp.models.meta_mtp.adapter import MetaLlamaMTPAdapter
from weighted_mtp.models.tokenizer_utils import load_tokenizer_from_config
from weighted_mtp.pipelines.checkpoint_utils import save_checkpoint
from weighted_mtp.pipelines.metrics_utils import (
    GPUMonitor,
    ThroughputTracker,
    compute_gradient_norm,
    get_model_size,
    get_system_info,
)
from weighted_mtp.runtime import (
    init_distributed,
    setup_environment,
    is_main_process,
    wrap_model_ddp,
    unwrap_model,
    all_reduce_scalar,
)
from weighted_mtp.value_weighting.rho1_weighting import (
    build_weights,
    compute_excess_loss,
    compute_rho1_stats,
)


def load_adapter(config: dict, device: torch.device) -> MetaLlamaMTPAdapter:
    """Adapter ë¡œë“œ

    Args:
        config: ëª¨ë¸ ì„¤ì •
        device: ë””ë°”ì´ìŠ¤

    Returns:
        MetaLlamaMTPAdapter ì¸ìŠ¤í„´ìŠ¤
    """
    adapter = MetaLlamaMTPAdapter.from_pretrained(
        model_path=config.models.policy.path,
        device=device,
        initialize_value_head=False,  # Rho-1ì€ Value Head ë¶ˆí•„ìš”
    )
    return adapter


def load_reference_model(config: dict, device: torch.device) -> MetaLlamaMTPAdapter:
    """Reference model ë¡œë“œ (ì»¤ìŠ¤í…€ Meta LLaMA MTP ëª¨ë¸)

    Args:
        config: ëª¨ë¸ ì„¤ì •
        device: ë””ë°”ì´ìŠ¤

    Returns:
        Reference model (eval mode, MetaLlamaMTPAdapter)
    """
    logger.info(f"Loading reference model: {config.models.reference.name}")
    logger.info(f"Path: {config.models.reference.path}")

    # MetaLlamaMTPAdapterë¡œ ë¡œë“œ (Value head ë¶ˆí•„ìš”)
    ref_model = MetaLlamaMTPAdapter.from_pretrained(
        model_path=config.models.reference.path,
        device=device,
        dtype=config.models.reference.dtype,
        initialize_value_head=False,
    )

    # Eval mode (gradient ë¶ˆí•„ìš”)
    ref_model.eval()

    # Gradient ê³„ì‚° ë¹„í™œì„±í™”
    for param in ref_model.parameters():
        param.requires_grad = False

    logger.info("âœ“ Reference model loaded successfully")

    return ref_model




def create_dataloader(
    dataset_path: str,
    tokenizer: AutoTokenizer,
    batch_size: int,
    max_length: int,
    n_samples: int | None,
    balance_correct: bool,
    correct_ratio: float,
    seed: int,
    shuffle: bool = True,
) -> DataLoader:
    """DataLoader ìƒì„± (Config-driven ìƒ˜í”Œë§)

    Args:
        dataset_path: ë°ì´í„°ì…‹ ê²½ë¡œ
        tokenizer: Tokenizer
        batch_size: ë°°ì¹˜ í¬ê¸°
        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        n_samples: ìƒ˜í”Œ ìˆ˜
        balance_correct: is_correct ê· í˜• ì—¬ë¶€ (Rho-1ì€ False)
        correct_ratio: correct ìƒ˜í”Œ ë¹„ìœ¨ (Rho-1ì€ 1.0)
        seed: ì‹œë“œ
        shuffle: ì…”í”Œ ì—¬ë¶€

    Returns:
        DataLoader
    """
    # ë°ì´í„°ì…‹ ì´ë¦„ ë° ìŠ¤í”Œë¦¿ ì¶”ì¶œ
    # storage/datasets_v2/codecontests/processed/train.jsonl -> codecontests
    dataset_path_obj = Path(dataset_path)
    dataset_name = dataset_path_obj.parent.parent.name
    split_file = dataset_path_obj.name

    if "train" in split_file:
        split = "train"
    elif "valid" in split_file or "validation" in split_file:
        split = "validation"
    else:
        split = "test"

    # ë°ì´í„°ì…‹ ë¡œë“œ (Config-driven ìƒ˜í”Œë§)
    dataset = load_dataset(
        dataset_name=dataset_name,
        split=split,
        n_samples=n_samples,
        balance_correct=balance_correct,
        correct_ratio=correct_ratio,
        difficulty_weights=None,  # Rho-1ì€ curriculum learning ì—†ìŒ
        difficulty_bins=None,
        seed=seed,
    )

    # Collator ìƒì„±
    collator = AlpacaDataCollator(
        tokenizer=tokenizer,
        max_length=max_length,
    )

    # DataLoader ìƒì„±
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        collate_fn=collator,
        num_workers=0,
    )

    return dataloader


def validate_rho1(
    adapter: MetaLlamaMTPAdapter,
    ref_model: MetaLlamaMTPAdapter,
    dataloader: DataLoader,
    device: torch.device,
    temperature: float,
) -> dict[str, float]:
    """Validation ìˆ˜í–‰ (Rho-1)

    Args:
        adapter: Adapter (DDP-wrapped ê°€ëŠ¥)
        ref_model: Reference model (MetaLlamaMTPAdapter, eval mode)
        dataloader: Validation DataLoader
        device: ë””ë°”ì´ìŠ¤
        temperature: Softmax temperature

    Returns:
        Validation metrics (DDP í™˜ê²½ì—ì„œëŠ” all-reduce ì ìš©ë¨)
    """
    # DDP unwrap for eval
    unwrapped_adapter = unwrap_model(adapter)
    unwrapped_adapter.eval()
    ref_model.eval()

    total_weighted_ce_loss = 0.0
    total_excess_loss = 0.0
    n_batches = 0

    with torch.no_grad():
        for batch in dataloader:
            # 1. Batchë¥¼ deviceë¡œ ì´ë™
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            # 2. Reference forward (NTP mode: ì²« ë²ˆì§¸ headë§Œ ì‚¬ìš©)
            ref_logits_mtp = ref_model.transformer.forward(input_ids, return_all_heads=False)
            ref_logits = ref_logits_mtp.squeeze(2)  # [batch, seq, 1, vocab] -> [batch, seq, vocab]

            # 3. Policy forward (MTP, value_head ë¶ˆí•„ìš”í•˜ë¯€ë¡œ transformer ì§ì ‘ í˜¸ì¶œ)
            policy_logits = unwrapped_adapter.transformer.forward(input_ids, return_all_heads=True)

            batch_size, seq_len, n_future, vocab_size = policy_logits.shape

            # 4. Excess loss ê³„ì‚°
            excess_loss = compute_excess_loss(
                policy_logits=policy_logits,
                ref_logits=ref_logits,
                labels=labels,
                attention_mask=attention_mask,
            )

            # 5. Rho-1 weights
            weights = build_weights(
                excess_loss=excess_loss,
                temperature=temperature,
                attention_mask=attention_mask,
            )

            # 6. Weighted CE loss (ëª¨ë“  Hê°œ í† í°)
            batch_weighted_ce_loss = 0.0

            for k in range(1, n_future + 1):
                valid_len = seq_len - k

                if valid_len <= 0:
                    continue

                policy_logits_k = policy_logits[:, :valid_len, k - 1, :]
                labels_k = labels[:, k : k + valid_len]
                weights_k = weights[:, :valid_len]
                mask_k = attention_mask[:, k : k + valid_len]

                ce_loss_k = F.cross_entropy(
                    policy_logits_k.reshape(-1, vocab_size),
                    labels_k.reshape(-1),
                    reduction="none",
                )

                weighted_ce_k = ce_loss_k * weights_k.reshape(-1) * mask_k.float().reshape(-1)

                mask_sum_k = mask_k.float().sum()
                if mask_sum_k > 0:
                    batch_weighted_ce_loss += weighted_ce_k.sum() / mask_sum_k

            weighted_ce_loss = batch_weighted_ce_loss / n_future

            # 7. Metrics ìˆ˜ì§‘
            total_weighted_ce_loss += weighted_ce_loss.item()
            total_excess_loss += excess_loss.mean().item()
            n_batches += 1

    # í‰ê·  metrics ê³„ì‚°
    avg_weighted_ce_loss = total_weighted_ce_loss / n_batches
    avg_excess_loss = total_excess_loss / n_batches

    # Validation metrics aggregation (DDP)
    avg_weighted_ce_loss = all_reduce_scalar(avg_weighted_ce_loss)
    avg_excess_loss = all_reduce_scalar(avg_excess_loss)

    metrics = {
        "val_weighted_ce_loss": avg_weighted_ce_loss,
        "val_excess_loss": avg_excess_loss,
        "val_loss": avg_weighted_ce_loss,  # Best trackingìš©
    }

    return metrics


def cleanup_old_checkpoints(
    checkpoint_dir: Path,
    save_total_limit: int,
) -> None:
    """ì˜¤ë˜ëœ ì¤‘ê°„ checkpoint ì‚­ì œ

    checkpoint_best.ptì™€ checkpoint_final.ptëŠ” ì ˆëŒ€ ì‚­ì œí•˜ì§€ ì•ŠìŒ
    checkpoint_epoch_*.ptë§Œ save_total_limit ê°œìˆ˜ë§Œí¼ ìœ ì§€

    Args:
        checkpoint_dir: Checkpoint ë””ë ‰í„°ë¦¬
        save_total_limit: ìœ ì§€í•  ìµœëŒ€ ê°œìˆ˜
    """
    if not checkpoint_dir.exists():
        return

    # ì¤‘ê°„ checkpoint íŒŒì¼ë§Œ ìˆ˜ì§‘ (checkpoint_epoch_*.pt)
    epoch_checkpoints = sorted(
        [f for f in checkpoint_dir.glob("checkpoint_epoch_*.pt")],
        key=lambda x: x.stat().st_mtime,  # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ì •ë ¬
    )

    # ì‚­ì œí•  íŒŒì¼ ê°œìˆ˜ ê³„ì‚°
    n_to_delete = len(epoch_checkpoints) - save_total_limit

    if n_to_delete > 0:
        for checkpoint_path in epoch_checkpoints[:n_to_delete]:
            logger.info(f"ì˜¤ë˜ëœ checkpoint ì‚­ì œ: {checkpoint_path.name}")
            checkpoint_path.unlink()


def run_rho1_training(config_path: str, **override_params: Any) -> tuple[dict[str, float], str]:
    """Rho-1 WMTP ì‹¤í–‰

    Args:
        config_path: configs/rho1/rho1.yaml
        override_params: CLI overrides

    Returns:
        (final_metrics, best_checkpoint_path)
    """
    # 0. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (MLflow credentials ë“±)
    ensure_env_loaded()

    # 1. Config ë¡œë”© (defaults + rho1 config merge)
    defaults = OmegaConf.load("configs/defaults.yaml")
    config = OmegaConf.load(config_path)
    config = OmegaConf.merge(defaults, config, override_params)

    # 2. Distributed ì´ˆê¸°í™” (torchrun í™˜ê²½ì¸ ê²½ìš°)
    if "RANK" in os.environ:
        rank, world_size = init_distributed()
    else:
        rank, world_size = 0, 1

    # 3. ë¡œê¹… ì„¤ì • (rank ì •ë³´ í¬í•¨)
    logger = setup_logging("RHO1", level=config.logging.level, rank=rank)

    logger.info("=== Rho-1 WMTP (Reference-based Weighting) ===")
    logger.info(f"Experiment: {config.experiment.name}")
    logger.info(f"Description: {config.experiment.description}")

    if "RANK" in os.environ:
        logger.info(f"Distributed training: rank={rank}, world_size={world_size}")
    else:
        logger.info("Local training (single device)")

    # 4. Environment setup (seed + device)
    actual_seed, device = setup_environment(config.runtime.seed)
    logger.info(f"Device: {device}, Seed: {actual_seed}")

    # 5. MLflow ì´ˆê¸°í™” (Rank 0ë§Œ)
    use_mlflow = bool(config.mlflow.experiment)
    if is_main_process() and use_mlflow:
        mlflow.set_tracking_uri(config.mlflow.tracking_uri)
        mlflow.set_experiment(config.mlflow.experiment)
        mlflow.start_run(
            run_name=config.experiment.name,
            tags={tag: "true" for tag in config.experiment.tags},
        )
        mlflow.log_params(OmegaConf.to_container(config, resolve=True))

    # 6. Resource ë¡œë”©
    adapter = load_adapter(config, device)
    ref_model = load_reference_model(config, device)
    tokenizer = load_tokenizer_from_config(config)

    # 7. DDP wrapping (adapterë§Œ - referenceëŠ” frozen inferenceìš©)
    adapter = wrap_model_ddp(adapter, device)

    # Model size + System info ë¡œê¹… (Rank 0ë§Œ)
    if is_main_process() and use_mlflow:
        model_size = get_model_size(unwrap_model(adapter))
        mlflow.log_params(
            {
                "model_total_params": model_size["total_params"],
                "model_trainable_params": model_size["trainable_params"],
            }
        )
        system_info = get_system_info()
        mlflow.log_params(
            {
                "system_cpu_count": system_info["cpu_count"],
                "system_ram_total_gb": round(system_info["ram_total_gb"], 2),
            }
        )

    # GPU monitor ì´ˆê¸°í™”
    gpu_monitor = GPUMonitor(device)
    throughput_tracker = ThroughputTracker()

    # 8. Dataset & DataLoader ìƒì„±
    logger.info(f"Dataset: {config.dataset.name}")
    logger.info(f"Train: {config.dataset.train}")
    logger.info(f"Validation: {config.dataset.validation}")

    train_loader = create_dataloader(
        dataset_path=config.dataset.train,
        tokenizer=tokenizer,
        batch_size=config.training.batch_size,
        max_length=config.dataset.max_length,
        n_samples=config.data_sampling.n_samples,
        balance_correct=config.data_sampling.balance_correct,
        correct_ratio=config.data_sampling.correct_ratio,
        seed=config.data_sampling.seed,
        shuffle=True,
    )

    # Validation ìƒ˜í”Œ ìˆ˜: trainì˜ 10% ë˜ëŠ” ìµœì†Œ 100ê°œ
    val_n_samples = max(100, config.data_sampling.n_samples // 10)

    val_loader = create_dataloader(
        dataset_path=config.dataset.validation,
        tokenizer=tokenizer,
        batch_size=config.training.batch_size,
        max_length=config.dataset.max_length,
        n_samples=val_n_samples,
        balance_correct=config.data_sampling.balance_correct,
        correct_ratio=config.data_sampling.correct_ratio,
        seed=config.data_sampling.seed,
        shuffle=False,
    )

    logger.info(f"Train batches: {len(train_loader)}")
    logger.info(f"Validation batches: {len(val_loader)}")

    # 6. Optimizer (MTP headsë§Œ - Value head ì—†ìŒ) - Meta MTP ë…¼ë¬¸ ì„¤ì •
    optimizer = torch.optim.AdamW(
        adapter.parameters(),
        lr=config.training.learning_rate,
        betas=(0.9, 0.95),
        weight_decay=0.01,
    )

    # 7. Training setup
    best_val_loss = float("inf")
    global_step = 0

    checkpoint_dir = Path(config.checkpoint.save_dir)
    checkpoint_dir.mkdir(parents=True, exist_ok=True)

    n_epochs = config.training.n_epochs
    save_checkpoint_every = config.checkpoint.save_checkpoint_every

    # Fractional epoch ì²˜ë¦¬
    total_batches = len(train_loader)
    batches_to_run = int(total_batches * n_epochs)

    logger.info(f"Total epochs: {n_epochs}")
    logger.info(f"Total batches to run: {batches_to_run}")
    logger.info(f"Validation & Checkpoint every: {save_checkpoint_every} epochs")
    logger.info(f"Temperature: {config.training.temperature}")

    current_epoch = 0.0
    batch_count = 0
    next_checkpoint_epoch = save_checkpoint_every

    # 8. Training loop
    while batch_count < batches_to_run:
        # Checkpoint ê²½ê³„ê¹Œì§€ í›ˆë ¨
        target_epoch = min(next_checkpoint_epoch, n_epochs)
        target_batches = int(target_epoch * total_batches)
        batches_this_period = target_batches - batch_count

        logger.info(f"--- Training to epoch {target_epoch:.2f} ---")

        # DataLoaderì—ì„œ í•„ìš”í•œ ë§Œí¼ë§Œ ì‚¬ìš©
        epoch_train_loader = iter(train_loader)
        period_metrics_sum = {"weighted_ce_loss": 0.0, "excess_loss": 0.0}
        period_batches = 0

        for _ in range(batches_this_period):
            try:
                batch = next(epoch_train_loader)
            except StopIteration:
                # DataLoader ì¬ì‹œì‘
                epoch_train_loader = iter(train_loader)
                batch = next(epoch_train_loader)

            # 1 batch í›ˆë ¨ (Rho-1 ë¡œì§)
            adapter.train()
            ref_model.eval()  # ReferenceëŠ” í•­ìƒ eval

            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            # Reference forward (no grad, NTP mode)
            with torch.no_grad():
                ref_logits_mtp = ref_model.transformer.forward(input_ids, return_all_heads=False)
                ref_logits = ref_logits_mtp.squeeze(2)  # [batch, seq, 1, vocab] -> [batch, seq, vocab]

            # Policy forward (MTP, value_head ë¶ˆí•„ìš”í•˜ë¯€ë¡œ transformer ì§ì ‘ í˜¸ì¶œ)
            policy_logits = adapter.transformer.forward(input_ids, return_all_heads=True)

            batch_size, seq_len, n_future, vocab_size = policy_logits.shape

            # Excess loss ê³„ì‚°
            excess_loss = compute_excess_loss(
                policy_logits=policy_logits,
                ref_logits=ref_logits,
                labels=labels,
                attention_mask=attention_mask,
            )

            # Rho-1 weights
            weights = build_weights(
                excess_loss=excess_loss,
                temperature=config.training.temperature,
                attention_mask=attention_mask,
            )

            # Weighted CE loss (ëª¨ë“  Hê°œ í† í°)
            batch_weighted_ce_loss = 0.0

            for k in range(1, n_future + 1):
                valid_len = seq_len - k

                if valid_len <= 0:
                    continue

                policy_logits_k = policy_logits[:, :valid_len, k - 1, :]
                labels_k = labels[:, k : k + valid_len]
                weights_k = weights[:, :valid_len]
                mask_k = attention_mask[:, k : k + valid_len]

                ce_loss_k = F.cross_entropy(
                    policy_logits_k.reshape(-1, vocab_size),
                    labels_k.reshape(-1),
                    reduction="none",
                )

                weighted_ce_k = ce_loss_k * weights_k.reshape(-1) * mask_k.float().reshape(-1)

                mask_sum_k = mask_k.float().sum()
                if mask_sum_k > 0:
                    batch_weighted_ce_loss += weighted_ce_k.sum() / mask_sum_k

            weighted_ce_loss = batch_weighted_ce_loss / n_future

            # Backward & update
            optimizer.zero_grad()
            weighted_ce_loss.backward()

            # Gradient clipping
            if config.training.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(
                    adapter.parameters(),
                    config.training.max_grad_norm,
                )

            optimizer.step()

            global_step += 1
            batch_count += 1
            period_batches += 1

            # Metrics ëˆ„ì 
            period_metrics_sum["weighted_ce_loss"] += weighted_ce_loss.item()
            period_metrics_sum["excess_loss"] += excess_loss.mean().item()

            # Step-level ë¡œê¹…
            if global_step % config.training.log_interval == 0:
                # Metric aggregation (DDP)
                avg_weighted_ce = all_reduce_scalar(weighted_ce_loss.item())
                avg_excess_loss = all_reduce_scalar(excess_loss.mean().item())

                if is_main_process():
                    if use_mlflow:
                        mlflow.log_metrics(
                            {
                                "train/weighted_ce_loss": avg_weighted_ce,
                                "train/excess_loss": avg_excess_loss,
                            },
                            step=global_step,
                        )
                    logger.info(
                        f"Step {global_step}/{batches_to_run}, "
                        f"Weighted CE: {avg_weighted_ce:.4f}, "
                        f"Excess Loss: {avg_excess_loss:.4f}"
                    )

        # Epoch ê²½ê³„ ë„ë‹¬
        current_epoch = batch_count / total_batches

        # Period-level metrics ê³„ì‚°
        train_weighted_ce_avg = period_metrics_sum["weighted_ce_loss"] / period_batches
        train_excess_avg = period_metrics_sum["excess_loss"] / period_batches

        logger.info(
            f"Epoch {current_epoch:.2f} ë„ë‹¬ - "
            f"Train Weighted CE: {train_weighted_ce_avg:.4f}"
        )

        # Validation ì‹¤í–‰ (epoch ê²½ê³„ì—ì„œ)
        logger.info(f"--- Validation at epoch {current_epoch:.2f} ---")
        val_metrics = validate_rho1(
            adapter=adapter,
            ref_model=ref_model,
            dataloader=val_loader,
            device=device,
            temperature=config.training.temperature,
        )

        # Epoch-level ë¡œê¹… (Rank 0ë§Œ)
        if is_main_process():
            if use_mlflow:
                mlflow.log_metrics(
                    {
                        "train/epoch_weighted_ce_loss": train_weighted_ce_avg,
                        "train/epoch_excess_loss": train_excess_avg,
                        "val/weighted_ce_loss": val_metrics["val_weighted_ce_loss"],
                        "val/excess_loss": val_metrics["val_excess_loss"],
                    },
                    step=int(current_epoch * 100),
                )

            logger.info(
                f"Validation - Weighted CE: {val_metrics['val_weighted_ce_loss']:.4f}, "
                f"Excess Loss: {val_metrics['val_excess_loss']:.4f}"
            )

        # Checkpoint ì €ì¥ (validation loss ê°œì„  ì‹œë§Œ)
        if val_metrics["val_loss"] < best_val_loss:
            best_val_loss = val_metrics["val_loss"]
            checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{current_epoch:.2f}.pt"

            save_checkpoint(
                adapter=unwrap_model(adapter),
                optimizer=optimizer,
                epoch=current_epoch,
                train_metrics={
                    "train_weighted_ce_loss": train_weighted_ce_avg,
                    "train_excess_loss": train_excess_avg,
                },
                val_metrics=val_metrics,
                checkpoint_path=checkpoint_path,
            )

            logger.info(f"âœ“ Improved checkpoint saved: {checkpoint_path.name} (val_loss: {best_val_loss:.4f})")

            # ì˜¤ë˜ëœ checkpoint ì •ë¦¬ (ìµœëŒ€ 3ê°œ ìœ ì§€)
            if config.checkpoint.get("save_total_limit"):
                cleanup_old_checkpoints(
                    checkpoint_dir=checkpoint_dir,
                    save_total_limit=config.checkpoint.save_total_limit,
                )
        else:
            logger.info(f"Validation loss did not improve ({val_metrics['val_loss']:.4f} >= {best_val_loss:.4f}), skipping checkpoint save")

        # ë‹¤ìŒ checkpoint ê²½ê³„ ì„¤ì •
        next_checkpoint_epoch += save_checkpoint_every

    # 9. Final checkpoint
    if config.checkpoint.save_final:
        final_path = checkpoint_dir / "checkpoint_final.pt"

        # ìµœì¢… validation ì‹¤í–‰
        logger.info("--- Final Validation ---")
        final_val_metrics = validate_rho1(
            adapter=adapter,
            ref_model=ref_model,
            dataloader=val_loader,
            device=device,
            temperature=config.training.temperature,
        )

        save_checkpoint(
            adapter=unwrap_model(adapter),
            optimizer=optimizer,
            epoch=current_epoch,
            train_metrics={
                "train_weighted_ce_loss": train_weighted_ce_avg,
                "train_excess_loss": train_excess_avg,
            },
            val_metrics=final_val_metrics,
            checkpoint_path=final_path,
        )

        logger.info(f"Final checkpoint saved: {final_path.name}")

    # 10. MLflow artifact ì—…ë¡œë“œ (Rank 0ë§Œ)
    if is_main_process() and use_mlflow:
        # ìµœì‹  epoch checkpoint ì—…ë¡œë“œ (ëª¨ë‘ validation loss ê°œì„  ì‹œì—ë§Œ ì €ì¥ë˜ë¯€ë¡œ best)
        epoch_checkpoints = sorted(checkpoint_dir.glob("checkpoint_epoch_*.pt"))
        if epoch_checkpoints:
            latest_checkpoint = epoch_checkpoints[-1]
            mlflow.log_artifact(str(latest_checkpoint), "checkpoints")
            logger.info(f"Latest checkpoint uploaded to MLflow: {latest_checkpoint.name}")
        mlflow.end_run()

    # ìµœì‹  checkpoint ê²½ë¡œ ë°˜í™˜
    epoch_checkpoints = sorted(checkpoint_dir.glob("checkpoint_epoch_*.pt"))
    latest_checkpoint_path = str(epoch_checkpoints[-1]) if epoch_checkpoints else None

    logger.info(f"ğŸ‰ Rho-1 WMTP ì™„ë£Œ! Latest checkpoint: {latest_checkpoint_path}")

    return final_val_metrics, latest_checkpoint_path


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Rho-1 WMTP (Reference-based Weighting)")
    parser.add_argument(
        "--config",
        required=True,
        help="Config path (e.g., configs/rho1/rho1.yaml)",
    )
    parser.add_argument("--run-name", help="MLflow run name override")
    parser.add_argument("--device", help="Device override (cuda/cpu/mps)")
    args = parser.parse_args()

    overrides = {}
    if args.run_name:
        overrides["experiment.name"] = args.run_name
    if args.device:
        overrides["runtime.device"] = args.device

    run_rho1_training(args.config, **overrides)
